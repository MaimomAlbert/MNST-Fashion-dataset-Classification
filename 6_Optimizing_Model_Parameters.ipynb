{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Optimizing Model Parameters\n",
        "- Now that we have a model and data it’s time to train, validate and test our model by optimizing its parameters on our data.\n",
        "- Training a model is an iterative process; in each iteration the model makes a guess about the output, calculates the error in its guess (loss), collects the derivatives of the error with respect to its parameters, and **optimizes** these parameters using gradient descent."
      ],
      "metadata": {
        "id": "nV4K8tNjSzU2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LwgN-rcGSpfk",
        "outputId": "202a1c74-8010-4302-dda4-d8fd076243c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 26421880/26421880 [00:02<00:00, 11339197.03it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/FashionMNIST/raw/train-images-idx3-ubyte.gz to data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 29515/29515 [00:00<00:00, 404098.88it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4422102/4422102 [00:00<00:00, 6194704.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5148/5148 [00:00<00:00, 16247010.53it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor\n",
        "\n",
        "training_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=ToTensor()\n",
        ")\n",
        "\n",
        "test_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=ToTensor()\n",
        ")\n",
        "\n",
        "train_dataloader = DataLoader(training_data, batch_size=64)\n",
        "test_dataloader = DataLoader(test_data, batch_size=64)\n",
        "\n",
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.linear_relu_stack = nn.Sequential(\n",
        "            nn.Linear(28*28, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 10),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        logits = self.linear_relu_stack(x)\n",
        "        return logits\n",
        "\n",
        "model = NeuralNetwork()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hyperparameters\n",
        "- Hyperparameters are adjustable parameters that let us control the model optimization process.\n",
        "- Different hyperparameter values can impact model training and convergence rates.\n",
        "- We define the following hyperparameters for training:\n",
        "  - **Number of epochs:** The number of times to iterate over the dataset.\n",
        "  - **Batch Size:** The number of data samples propagated through the network before the parameters are updated.\n",
        "  - **Learning rate:** How much update we should apply to the model parameters at each batch/epoch. Smaller values yield slow learning speed, while large values may result in unpredictable behaviour during training."
      ],
      "metadata": {
        "id": "psHzThyVTdYH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 1e-3\n",
        "batch_size = 64\n",
        "epochs = 5"
      ],
      "metadata": {
        "id": "g7X25QQXTbXD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Optimization Loop\n",
        "- Once we set our hyperparameters, we can then train and optimize our model with an optimization loop. Each iteration of the optimization loop is called an **epoch**.\n",
        "- Each epoch consists of two main parts:\n",
        "  - **The Train Loop** - iterate over the training dataset and try to converge to optimal parameters.\n",
        "  - **The Validation/Test Loop** - iterate over the test dataset to check if model performance is improving.\n",
        "\n",
        "# Loss Function\n",
        "- Loss function measures the degree of dissimilarity of obtained result to the target value, and it is the loss function that we want to minimize during training. - To calculate the loss we make a prediction using the inputs of our given data sample and compare it against the true data label value.\n",
        "- Common loss functions include `nn.MSELoss (Mean Square Error)` for regression tasks, and `nn.NLLLoss (Negative Log Likelihood)` for classification. `nn.CrossEntropyLoss` combines `nn.LogSoftmax` and `nn.NLLLoss`.\n",
        "- We pass our model’s output logits to nn.CrossEntropyLoss, which will normalize the logits and compute the prediction error."
      ],
      "metadata": {
        "id": "RhXEsPyCUrwW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the loss function\n",
        "loss_fn = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "iPBhY-CSUq4x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Optimizer\n",
        "- Optimization is the process of adjusting model parameters to reduce model error in each training step.\n",
        "- **Optimization algorithms** define how this process is performed (in this example we use Stochastic Gradient Descent).\n",
        "- All optimization logic is encapsulated in the `optimizer` object.\n",
        "- Optimizers available in PyTorch includes: `SGD`, `ADAM`, and `RMSProp`.\n",
        "- We initialize the optimizer by registering the model’s parameters that need to be trained, and passing in the learning rate hyperparameter.\n"
      ],
      "metadata": {
        "id": "O0hXNOyCWCoA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "id": "hqCGseJuWBs0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inside the training loop, optimization happens in three steps:\n",
        "- Call `optimizer.zero_grad()` to reset the gradients of model parameters. Gradients by default add up; to prevent double-counting, we explicitly zero them at each iteration.\n",
        "- Backpropagate the prediction loss with a call to `loss.backward()`. PyTorch deposits the gradients of the loss w.r.t. each parameter.\n",
        "- Once we have our gradients, we call `optimizer.step()` to adjust the parameters by the gradients collected in the backward pass."
      ],
      "metadata": {
        "id": "ke9XVmNcW0uK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Full Implementation\n",
        "We define `train_loop` that loops over our optimization code, and `test_loop` that evaluates the model's performance against our test data."
      ],
      "metadata": {
        "id": "96StxMQpXRLJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_loop(dataloader, model, loss_fn, optimizer):\n",
        "  size = len(dataloader.dataset)\n",
        "  # Set the model to training mode - important for batch normalization and dropout layers\n",
        "  # Unnecessary in this situation but added for best practices\n",
        "  model.train()\n",
        "  for batch, (X, y) in enumerate(dataloader):\n",
        "    # Compute prediction and loss\n",
        "    pred = model(X)\n",
        "    loss = loss_fn(pred, y)\n",
        "\n",
        "    # Backpropagation\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    if batch%100 == 0:\n",
        "      loss, current = loss.item(), batch * batch_size + len(X)\n",
        "      print(f\"loss: {loss:>7f} [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "def test_loop(dataloader, model, loss_fn):\n",
        "    # Set the model to evaluation mode - important for batch normalization and dropout layers\n",
        "    # Unnecessary in this situation but added for best practices\n",
        "    model.eval()\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    test_loss, correct = 0, 0\n",
        "    # Evaluating the model with torch.no_grad() ensures that no gradients are computed during test mode\n",
        "    # also serves to avoid unnecessary gradient computations and memory usage for tensors with requires_grad=True\n",
        "    with torch.no_grad():\n",
        "      for X, y in dataloader:\n",
        "        pred = model(X)\n",
        "        test_loss += loss_fn(pred, y).item()\n",
        "        correct += (pred.argmax(dim=1) == y).type(torch.float).sum().item()\n",
        "    test_loss /= num_batches\n",
        "    correct /= size\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n"
      ],
      "metadata": {
        "id": "NaYQ8kttWzBI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate)\n",
        "epochs = 10\n",
        "\n",
        "for t in range(epochs):\n",
        "  print(f\"Epoch {t+1}\\n -----------------------\")\n",
        "  train_loop(train_dataloader, model, loss_fn, optimizer)\n",
        "  test_loop(test_dataloader, model, loss_fn)\n",
        "print(\"Done!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O208Ezq2akNx",
        "outputId": "6bd8ea8c-8554-4880-8b98-1a86c698b088"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            " -----------------------\n",
            "loss: 2.297175 [   64/60000]\n",
            "loss: 2.290338 [ 6464/60000]\n",
            "loss: 2.268400 [12864/60000]\n",
            "loss: 2.269954 [19264/60000]\n",
            "loss: 2.247679 [25664/60000]\n",
            "loss: 2.221757 [32064/60000]\n",
            "loss: 2.232344 [38464/60000]\n",
            "loss: 2.196507 [44864/60000]\n",
            "loss: 2.190325 [51264/60000]\n",
            "loss: 2.158698 [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 50.5%, Avg loss: 2.154517 \n",
            "\n",
            "Epoch 2\n",
            " -----------------------\n",
            "loss: 2.157152 [   64/60000]\n",
            "loss: 2.152339 [ 6464/60000]\n",
            "loss: 2.090851 [12864/60000]\n",
            "loss: 2.113436 [19264/60000]\n",
            "loss: 2.067412 [25664/60000]\n",
            "loss: 2.003962 [32064/60000]\n",
            "loss: 2.030914 [38464/60000]\n",
            "loss: 1.951374 [44864/60000]\n",
            "loss: 1.952869 [51264/60000]\n",
            "loss: 1.881200 [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 60.6%, Avg loss: 1.878923 \n",
            "\n",
            "Epoch 3\n",
            " -----------------------\n",
            "loss: 1.904539 [   64/60000]\n",
            "loss: 1.880034 [ 6464/60000]\n",
            "loss: 1.757731 [12864/60000]\n",
            "loss: 1.804962 [19264/60000]\n",
            "loss: 1.705439 [25664/60000]\n",
            "loss: 1.651705 [32064/60000]\n",
            "loss: 1.665070 [38464/60000]\n",
            "loss: 1.571437 [44864/60000]\n",
            "loss: 1.594999 [51264/60000]\n",
            "loss: 1.486912 [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 62.2%, Avg loss: 1.506680 \n",
            "\n",
            "Epoch 4\n",
            " -----------------------\n",
            "loss: 1.565557 [   64/60000]\n",
            "loss: 1.539008 [ 6464/60000]\n",
            "loss: 1.383350 [12864/60000]\n",
            "loss: 1.463087 [19264/60000]\n",
            "loss: 1.351510 [25664/60000]\n",
            "loss: 1.342004 [32064/60000]\n",
            "loss: 1.350308 [38464/60000]\n",
            "loss: 1.279477 [44864/60000]\n",
            "loss: 1.317135 [51264/60000]\n",
            "loss: 1.213813 [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 63.5%, Avg loss: 1.240484 \n",
            "\n",
            "Epoch 5\n",
            " -----------------------\n",
            "loss: 1.309463 [   64/60000]\n",
            "loss: 1.300413 [ 6464/60000]\n",
            "loss: 1.128027 [12864/60000]\n",
            "loss: 1.242751 [19264/60000]\n",
            "loss: 1.120667 [25664/60000]\n",
            "loss: 1.140654 [32064/60000]\n",
            "loss: 1.161640 [38464/60000]\n",
            "loss: 1.100831 [44864/60000]\n",
            "loss: 1.143812 [51264/60000]\n",
            "loss: 1.056552 [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 64.6%, Avg loss: 1.076668 \n",
            "\n",
            "Epoch 6\n",
            " -----------------------\n",
            "loss: 1.139727 [   64/60000]\n",
            "loss: 1.151121 [ 6464/60000]\n",
            "loss: 0.961897 [12864/60000]\n",
            "loss: 1.106215 [19264/60000]\n",
            "loss: 0.979299 [25664/60000]\n",
            "loss: 1.007201 [32064/60000]\n",
            "loss: 1.046763 [38464/60000]\n",
            "loss: 0.988715 [44864/60000]\n",
            "loss: 1.031534 [51264/60000]\n",
            "loss: 0.959502 [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 65.6%, Avg loss: 0.972247 \n",
            "\n",
            "Epoch 7\n",
            " -----------------------\n",
            "loss: 1.023439 [   64/60000]\n",
            "loss: 1.055604 [ 6464/60000]\n",
            "loss: 0.849959 [12864/60000]\n",
            "loss: 1.015904 [19264/60000]\n",
            "loss: 0.891382 [25664/60000]\n",
            "loss: 0.915160 [32064/60000]\n",
            "loss: 0.973143 [38464/60000]\n",
            "loss: 0.916836 [44864/60000]\n",
            "loss: 0.954880 [51264/60000]\n",
            "loss: 0.895224 [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 66.9%, Avg loss: 0.902069 \n",
            "\n",
            "Epoch 8\n",
            " -----------------------\n",
            "loss: 0.939296 [   64/60000]\n",
            "loss: 0.990419 [ 6464/60000]\n",
            "loss: 0.770894 [12864/60000]\n",
            "loss: 0.952442 [19264/60000]\n",
            "loss: 0.833741 [25664/60000]\n",
            "loss: 0.849285 [32064/60000]\n",
            "loss: 0.922465 [38464/60000]\n",
            "loss: 0.869549 [44864/60000]\n",
            "loss: 0.900206 [51264/60000]\n",
            "loss: 0.849354 [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 67.9%, Avg loss: 0.852142 \n",
            "\n",
            "Epoch 9\n",
            " -----------------------\n",
            "loss: 0.875174 [   64/60000]\n",
            "loss: 0.941982 [ 6464/60000]\n",
            "loss: 0.712538 [12864/60000]\n",
            "loss: 0.905516 [19264/60000]\n",
            "loss: 0.793422 [25664/60000]\n",
            "loss: 0.800588 [32064/60000]\n",
            "loss: 0.884418 [38464/60000]\n",
            "loss: 0.836821 [44864/60000]\n",
            "loss: 0.859596 [51264/60000]\n",
            "loss: 0.814793 [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 69.3%, Avg loss: 0.814624 \n",
            "\n",
            "Epoch 10\n",
            " -----------------------\n",
            "loss: 0.824190 [   64/60000]\n",
            "loss: 0.903433 [ 6464/60000]\n",
            "loss: 0.667390 [12864/60000]\n",
            "loss: 0.869425 [19264/60000]\n",
            "loss: 0.763500 [25664/60000]\n",
            "loss: 0.763686 [32064/60000]\n",
            "loss: 0.853559 [38464/60000]\n",
            "loss: 0.812735 [44864/60000]\n",
            "loss: 0.828071 [51264/60000]\n",
            "loss: 0.786914 [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 70.5%, Avg loss: 0.784830 \n",
            "\n",
            "Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uodKf3FCbFwS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}